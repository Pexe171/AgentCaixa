# Assistente RAG com Feedback de Aprendizado (Fases 1 a 6)

Este projeto implementa um pipeline completo de perguntas e respostas sobre documentos `.docx`, com busca h√≠brida, gera√ß√£o Multi-LLM (Ollama, OpenAI e Gemini), reescrita de perguntas para melhorar recupera√ß√£o no banco vetorial, interface de chat no Streamlit com coleta de feedback e avaliador em lote para valida√ß√£o massiva.

## Vis√£o geral da arquitetura

- **Fase 1 ‚Äî Ingest√£o (`ingest_docx.py`)**
  - Extrai texto de par√°grafos e tabelas de arquivos `.docx`.
  - Gera chunks conservadores (at√© 400 caracteres com overlap de 150).
  - Salva os chunks em JSON para auditoria e reuso.
- **Fase 2 ‚Äî Recupera√ß√£o h√≠brida (`retriever.py`)**
  - Indexa chunks no ChromaDB local (persistente em disco).
  - Realiza indexa√ß√£o em lotes de 50 chunks para reduzir timeout no embedding via Ollama.
  - Usa embedding via Ollama (`nomic-embed-text` por padr√£o).
  - Combina busca vetorial + BM25 com fus√£o RRF ponderada.
- **Fase 3 ‚Äî Resposta final (`agent.py`)**
  - Recupera os melhores trechos via retriever h√≠brido.
  - Carrega o prompt de sistema a partir de ficheiros externos em `prompts/` (padr√£o: `especialista_habitacional.txt`).
  - Permite trocar o especialista via argumento `--prompt-sistema`.
  - Suporta arquitetura h√≠brida com `Ollama`, `OpenAI` e `Google Gemini`, sempre com temperatura 0.0 e prompts externos em `prompts/`.
- **Fase 4 ‚Äî Interface (`app.py`)**
  - Chat humanizado em Streamlit.
  - Para cada resposta do bot: bot√µes **üëç Correto** e **üëé Impreciso**.
  - Salva feedback em SQLite (`feedback.db`) com data, pergunta, resposta e feedback (1/0).
  - Exibe na sidebar o **Gr√°fico de Aprendizado** com taxa de acerto (%) ao longo do tempo.
- **Fase 5 ‚Äî Avaliador em lote (`avaliador_em_lote.py`)**
  - L√™ `perguntas.txt` (uma pergunta por linha).
  - Recupera contexto com `HybridRetriever` usando **Top-K=4** (padr√£o).
  - Gera resposta para cada pergunta com roteamento autom√°tico por provedor (`gerar_resposta_hibrida`).
  - Para OpenAI e Gemini, usa paralelismo por threads para acelerar a gera√ß√£o do Relat√≥rio de Ouro.
  - Exporta CSV com colunas para auditoria e avalia√ß√£o manual.
- **Fase 6 ‚Äî Query Rewriting (`query_rewriter.py`)**
  - Reescreve perguntas coloquiais para uma vers√£o t√©cnica focada em normas habitacionais da Caixa, usando prompt externo `prompts/reescritor_tecnico.txt`.
  - Mant√©m cache em mem√≥ria das perguntas reescritas para reduzir lat√™ncia e chamadas repetidas ao Ollama.
  - Usa chamada r√°pida ao endpoint `http://localhost:11434/api/generate` com `requests` e timeout de 10s.
  - Em caso de erro/timeout, retorna a pergunta original como fallback seguro.

---


## üõ†Ô∏è Configura√ß√£o de Motores de IA

O AgentCaixa agora suporta tr√™s provedores de gera√ß√£o: **Ollama (local)**, **OpenAI** e **Google Gemini**.

### 1) Obter chave da OpenAI

1. Acesse o painel da OpenAI.
2. Gere uma API key em **API Keys**.
3. Copie a chave para uso no `.env`.

### 2) Obter chave do Google AI Studio (Gemini)

1. Acesse o Google AI Studio.
2. Crie uma API key para a API Gemini.
3. Copie a chave para uso no `.env`.

### 3) Configurar o arquivo `.env`

Na raiz do projeto, crie (ou edite) o arquivo `.env` com as duas chaves:

```bash
OPENAI_API_KEY=sua_chave_openai_aqui
GOOGLE_API_KEY=sua_chave_google_ai_studio_aqui
```

> O carregamento do `.env` √© autom√°tico nas integra√ß√µes cloud do sistema (OpenAI/Gemini).

### 4) Executar avaliador em lote por provedor

```bash
# Ollama (local)
python avaliador_em_lote.py --provedor local --modelo-llm llama3

# OpenAI (Relat√≥rio de Ouro com threads)
python avaliador_em_lote.py --provedor openai --threads 50 --modelo-llm gpt-4o-mini

# Gemini (Relat√≥rio de Ouro com threads)
python avaliador_em_lote.py --provedor gemini --threads 50 --modelo-llm gemini-1.5-flash
```

## Pr√©-requisitos

- Python **3.10+**
- Ollama instalado e em execu√ß√£o local
- Pacote Python `ollama` (usado internamente pelo ChromaDB para embeddings via Ollama)
- Modelos Ollama dispon√≠veis:
  - embeddings: `nomic-embed-text`
  - gera√ß√£o: `llama3`

Exemplo para preparar modelos no Ollama:

```bash
ollama pull nomic-embed-text
ollama pull llama3
```

---

## Instala√ß√£o

No diret√≥rio do projeto:

```bash
pip install python-docx chromadb rank-bm25 requests streamlit pandas ollama openai google-generativeai python-dotenv
```

---

## Execu√ß√£o do sistema completo

### 1) Gerar chunks do documento (Fase 1)

```bash
python ingest_docx.py caminho/arquivo.docx --saida ./chunks_auditoria.json
```

### 2) Indexar chunks no ChromaDB (Fase 2)

```bash
python retriever.py --chunks-json ./chunks_auditoria.json --limpar
```

> Dica: ajuste o tamanho de lote de indexa√ß√£o (padr√£o 50) com `--lote-indexacao` quando precisar otimizar estabilidade de embeddings.

### 3) (Opcional) Testar resposta via CLI (Fase 3)

```bash
python agent.py --pergunta "Qual √© a vig√™ncia da norma X?"
```

Tamb√©m √© poss√≠vel controlar o lote de indexa√ß√£o no fluxo da Fase 3 com `--lote-indexacao 50`.

### 4) Subir a interface web (Fase 4 + Auditoria visual da Fase 5)

```bash
streamlit run app.py
```

Depois, abra no navegador o endere√ßo mostrado pelo Streamlit (normalmente `http://localhost:8501`).

### 5) Rodar avaliador em lote (Fase 5)

Crie um arquivo `perguntas.txt` com uma pergunta por linha e execute:

```bash
python avaliador_em_lote.py --provedor local
```

Sa√≠da padr√£o local: `relatorio_avaliacao.csv`.

#### Modo Turbo (OpenAI com paralelismo)

```bash
python avaliador_em_lote.py --provedor openai --threads 50 --modelo-llm gpt-4o-mini
```

Sa√≠da padr√£o OpenAI: `relatorio_ouro_openai.csv`.


### 6) Auditar manualmente o relat√≥rio no Streamlit

1. Na barra lateral, selecione **Auditoria de Lote** em **Navega√ß√£o**.
2. O app carregar√° `relatorio_avaliacao.csv` automaticamente.
3. Edite apenas a coluna **Avalia√ß√£o Manual** usando as op√ß√µes:
   - (vazio)
   - üëç Correto
   - üëé Incorreto
4. Clique em **Salvar Avalia√ß√µes** para sobrescrever o CSV com suas marca√ß√µes.

---


### Prompts externos especializados

A pasta `prompts/` centraliza os prompts de sistema e elimina textos fixos no c√≥digo Python:

- `especialista_habitacional.txt` (padr√£o da Fase 3)
- `especialista_renda.txt`
- `reescritor_tecnico.txt` (Fase 6)

Para trocar o especialista na gera√ß√£o final (Fase 3), use:

```bash
python agent.py --pergunta "Minha pergunta" --prompt-sistema especialista_renda.txt
```

## Como usar o chat

1. Abra o app com `streamlit run app.py`.
2. Na **sidebar**, ajuste configura√ß√µes como diret√≥rio do Chroma, cole√ß√£o e modelos.
3. Selecione o **Provedor de IA** em `Ollama`, `OpenAI` ou `Gemini`.
4. O app atualiza dinamicamente a lista de **Modelo LLM** conforme o provedor escolhido.
5. Quando `OpenAI` ou `Gemini` estiver ativo, o app exibir√° o aviso **Custo por token ativo**.
6. O campo **Top-K de contexto** inicia em `4` por padr√£o (para reduzir lat√™ncia); diminua para `3` se quiser ainda mais velocidade.
7. Digite sua pergunta no campo de chat. Antes da busca, o sistema aplica automaticamente Query Rewriting para melhorar a recupera√ß√£o de contexto.
8. Ap√≥s cada resposta, clique em:
   - **üëç Correto** quando a resposta estiver adequada.
   - **üëé Impreciso** quando estiver incorreta ou incompleta.
9. A sidebar atualiza o **Gr√°fico de Aprendizado** com a taxa de acerto (%) por data.

---

## Banco de feedback (`feedback.db`)

A tabela `feedback` armazena:

- `data_hora` (timestamp da avalia√ß√£o)
- `pergunta`
- `resposta`
- `feedback` (`1` para üëç e `0` para üëé)
- `message_id` (identificador √∫nico da resposta para evitar duplicidade)

Esse banco √© criado automaticamente na primeira execu√ß√£o do `app.py`.

---

## Estrutura dos arquivos principais

- `ingest_docx.py` ‚Äî ingest√£o e chunking de `.docx`
- `retriever.py` ‚Äî indexa√ß√£o e busca h√≠brida (vetorial + BM25)
- `agent.py` ‚Äî gera√ß√£o final com roteamento h√≠brido entre Ollama, OpenAI e Gemini, com carregamento de prompt externo
- `app.py` ‚Äî interface Streamlit com dois modos: **Chatbot** e **Auditoria de Lote**, incluindo edi√ß√£o/salvamento da coluna `Avalia√ß√£o Manual` no CSV
- `avaliador_em_lote.py` ‚Äî execu√ß√£o em lote para valida√ß√£o e auditoria de respostas (inclui modo concorrente para OpenAI/Gemini)
- `query_rewriter.py` ‚Äî reescrita t√©cnica de perguntas com prompt externo (Query Rewriting)
- `prompts/` ‚Äî prompts de sistema especializados por dom√≠nio
- `feedback.db` ‚Äî banco SQLite gerado em runtime
- `perguntas.txt` ‚Äî arquivo de entrada (uma pergunta por linha) para a Fase 5
- `relatorio_avaliacao.csv` ‚Äî relat√≥rio gerado pela Fase 5

---

## Solu√ß√£o de problemas

- **Erro ao conectar no Ollama**
  - Verifique se o Ollama est√° ativo e acess√≠vel em `http://localhost:11434`.
- **Sem resultados na busca**
  - Reindexe com `--limpar` para reconstruir a base vetorial e BM25.
- **Gr√°fico de aprendizado vazio**
  - √â esperado at√© existir pelo menos um feedback registrado.

---

## Pr√≥ximas evolu√ß√µes recomendadas

- Filtro por cole√ß√£o/documento no chat.
- Dashboard com distribui√ß√£o de feedback por tema.
- Exporta√ß√£o de feedback para CSV e rotinas de melhoria cont√≠nua do prompt.

---

## Melhorias recentes de intelig√™ncia, performance e robustez

- **Mais r√°pido:** adicionado cache de Query Rewriting para perguntas repetidas, evitando chamadas redundantes ao Ollama.
- **Mais robusto:** valida√ß√µes extras no retriever h√≠brido (`k_rrf > 0` e ao menos um peso de busca maior que zero).
- **Mais resiliente:** alinhamento entre documentos, IDs e metadados no BM25 para evitar inconsist√™ncias quando houver itens inv√°lidos.
- **Mais previs√≠vel para o usu√°rio:** quando nenhum contexto √© recuperado no chat, o sistema retorna imediatamente `[Informa√ß√£o n√£o encontrada no documento]`.
