# Assistente RAG com Feedback de Aprendizado (Fases 1 a 4)

Este projeto implementa um pipeline completo de perguntas e respostas sobre documentos `.docx`, com busca h√≠brida, gera√ß√£o com LLM local e interface de chat no Streamlit com coleta de feedback.

## Vis√£o geral da arquitetura

- **Fase 1 ‚Äî Ingest√£o (`ingest_docx.py`)**
  - Extrai texto de par√°grafos e tabelas de arquivos `.docx`.
  - Gera chunks conservadores (at√© 400 caracteres com overlap de 150).
  - Salva os chunks em JSON para auditoria e reuso.
- **Fase 2 ‚Äî Recupera√ß√£o h√≠brida (`retriever.py`)**
  - Indexa chunks no ChromaDB local (persistente em disco).
  - Realiza indexa√ß√£o em lotes de 50 chunks para reduzir timeout no embedding via Ollama.
  - Usa embedding via Ollama (`nomic-embed-text` por padr√£o).
  - Combina busca vetorial + BM25 com fus√£o RRF ponderada.
- **Fase 3 ‚Äî Resposta final (`agent.py`)**
  - Recupera os melhores trechos via retriever h√≠brido.
  - Monta prompt estrito e chama Ollama (`llama3`, temperatura 0.0) com timeout padr√£o de 600 segundos.
  - Responde apenas com base no contexto recuperado.
- **Fase 4 ‚Äî Interface (`app.py`)**
  - Chat humanizado em Streamlit.
  - Para cada resposta do bot: bot√µes **üëç Correto** e **üëé Impreciso**.
  - Salva feedback em SQLite (`feedback.db`) com data, pergunta, resposta e feedback (1/0).
  - Exibe na sidebar o **Gr√°fico de Aprendizado** com taxa de acerto (%) ao longo do tempo.

---

## Pr√©-requisitos

- Python **3.10+**
- Ollama instalado e em execu√ß√£o local
- Pacote Python `ollama` (usado internamente pelo ChromaDB para embeddings via Ollama)
- Modelos Ollama dispon√≠veis:
  - embeddings: `nomic-embed-text`
  - gera√ß√£o: `llama3`

Exemplo para preparar modelos no Ollama:

```bash
ollama pull nomic-embed-text
ollama pull llama3
```

---

## Instala√ß√£o

No diret√≥rio do projeto:

```bash
pip install python-docx chromadb rank-bm25 requests streamlit pandas ollama
```

---

## Execu√ß√£o do sistema completo

### 1) Gerar chunks do documento (Fase 1)

```bash
python ingest_docx.py caminho/arquivo.docx --saida ./chunks_auditoria.json
```

### 2) Indexar chunks no ChromaDB (Fase 2)

```bash
python retriever.py --chunks-json ./chunks_auditoria.json --limpar
```

> Dica: ajuste o tamanho de lote de indexa√ß√£o (padr√£o 50) com `--lote-indexacao` quando precisar otimizar estabilidade de embeddings.

### 3) (Opcional) Testar resposta via CLI (Fase 3)

```bash
python agent.py --pergunta "Qual √© a vig√™ncia da norma X?"
```

Tamb√©m √© poss√≠vel controlar o lote de indexa√ß√£o no fluxo da Fase 3 com `--lote-indexacao 50`.

### 4) Subir a interface web (Fase 4)

```bash
streamlit run app.py
```

Depois, abra no navegador o endere√ßo mostrado pelo Streamlit (normalmente `http://localhost:8501`).

---

## Como usar o chat

1. Abra o app com `streamlit run app.py`.
2. Na **sidebar**, ajuste configura√ß√µes como diret√≥rio do Chroma, cole√ß√£o e modelos do Ollama.
3. O campo **Top-K de contexto** inicia em `4` por padr√£o (para reduzir lat√™ncia); diminua para `3` se quiser ainda mais velocidade.
4. Digite sua pergunta no campo de chat.
5. Ap√≥s cada resposta, clique em:
   - **üëç Correto** quando a resposta estiver adequada.
   - **üëé Impreciso** quando estiver incorreta ou incompleta.
6. A sidebar atualiza o **Gr√°fico de Aprendizado** com a taxa de acerto (%) por data.

---

## Banco de feedback (`feedback.db`)

A tabela `feedback` armazena:

- `data_hora` (timestamp da avalia√ß√£o)
- `pergunta`
- `resposta`
- `feedback` (`1` para üëç e `0` para üëé)
- `message_id` (identificador √∫nico da resposta para evitar duplicidade)

Esse banco √© criado automaticamente na primeira execu√ß√£o do `app.py`.

---

## Estrutura dos arquivos principais

- `ingest_docx.py` ‚Äî ingest√£o e chunking de `.docx`
- `retriever.py` ‚Äî indexa√ß√£o e busca h√≠brida (vetorial + BM25)
- `agent.py` ‚Äî gera√ß√£o final de resposta com Ollama
- `app.py` ‚Äî interface Streamlit e coleta de feedback
- `feedback.db` ‚Äî banco SQLite gerado em runtime

---

## Solu√ß√£o de problemas

- **Erro ao conectar no Ollama**
  - Verifique se o Ollama est√° ativo e acess√≠vel em `http://localhost:11434`.
- **Sem resultados na busca**
  - Reindexe com `--limpar` para reconstruir a base vetorial e BM25.
- **Gr√°fico de aprendizado vazio**
  - √â esperado at√© existir pelo menos um feedback registrado.

---

## Pr√≥ximas evolu√ß√µes recomendadas

- Filtro por cole√ß√£o/documento no chat.
- Dashboard com distribui√ß√£o de feedback por tema.
- Exporta√ß√£o de feedback para CSV e rotinas de melhoria cont√≠nua do prompt.
