# Assistente RAG com Feedback de Aprendizado (Fases 1 a 6)

Este projeto implementa um pipeline completo de perguntas e respostas sobre documentos `.docx`, com busca h√≠brida, gera√ß√£o com LLM local, reescrita de perguntas para melhorar recupera√ß√£o no banco vetorial, interface de chat no Streamlit com coleta de feedback e avaliador em lote para valida√ß√£o massiva.

## Vis√£o geral da arquitetura

- **Fase 1 ‚Äî Ingest√£o (`ingest_docx.py`)**
  - Extrai texto de par√°grafos e tabelas de arquivos `.docx`.
  - Gera chunks conservadores (at√© 400 caracteres com overlap de 150).
  - Salva os chunks em JSON para auditoria e reuso.
- **Fase 2 ‚Äî Recupera√ß√£o h√≠brida (`retriever.py`)**
  - Indexa chunks no ChromaDB local (persistente em disco).
  - Realiza indexa√ß√£o em lotes de 50 chunks para reduzir timeout no embedding via Ollama.
  - Usa embedding via Ollama (`nomic-embed-text` por padr√£o).
  - Combina busca vetorial + BM25 com fus√£o RRF ponderada.
- **Fase 3 ‚Äî Resposta final (`agent.py`)**
  - Recupera os melhores trechos via retriever h√≠brido.
  - Carrega o prompt de sistema a partir de ficheiros externos em `prompts/` (padr√£o: `especialista_habitacional.txt`).
  - Permite trocar o especialista via argumento `--prompt-sistema`.
  - Chama Ollama (`llama3`, temperatura 0.0) com timeout padr√£o de 600 segundos e responde apenas com base no contexto recuperado.
- **Fase 4 ‚Äî Interface (`app.py`)**
  - Chat humanizado em Streamlit.
  - Para cada resposta do bot: bot√µes **üëç Correto** e **üëé Impreciso**.
  - Salva feedback em SQLite (`feedback.db`) com data, pergunta, resposta e feedback (1/0).
  - Exibe na sidebar o **Gr√°fico de Aprendizado** com taxa de acerto (%) ao longo do tempo.
- **Fase 5 ‚Äî Avaliador em lote (`avaliador_em_lote.py`)**
  - L√™ `perguntas.txt` (uma pergunta por linha).
  - Recupera contexto com `HybridRetriever` usando **Top-K=4** (padr√£o).
  - Gera resposta para cada pergunta com `responder_com_ollama`.
  - Exporta `relatorio_avaliacao.csv` com colunas para auditoria e avalia√ß√£o manual.
- **Fase 6 ‚Äî Query Rewriting (`query_rewriter.py`)**
  - Reescreve perguntas coloquiais para uma vers√£o t√©cnica focada em normas habitacionais da Caixa, usando prompt externo `prompts/reescritor_tecnico.txt`.
  - Mant√©m cache em mem√≥ria das perguntas reescritas para reduzir lat√™ncia e chamadas repetidas ao Ollama.
  - Usa chamada r√°pida ao endpoint `http://localhost:11434/api/generate` com `requests` e timeout de 10s.
  - Em caso de erro/timeout, retorna a pergunta original como fallback seguro.

---

## Pr√©-requisitos

- Python **3.10+**
- Ollama instalado e em execu√ß√£o local
- Pacote Python `ollama` (usado internamente pelo ChromaDB para embeddings via Ollama)
- Modelos Ollama dispon√≠veis:
  - embeddings: `nomic-embed-text`
  - gera√ß√£o: `llama3`

Exemplo para preparar modelos no Ollama:

```bash
ollama pull nomic-embed-text
ollama pull llama3
```

---

## Instala√ß√£o

No diret√≥rio do projeto:

```bash
pip install python-docx chromadb rank-bm25 requests streamlit pandas ollama
```

---

## Execu√ß√£o do sistema completo

### 1) Gerar chunks do documento (Fase 1)

```bash
python ingest_docx.py caminho/arquivo.docx --saida ./chunks_auditoria.json
```

### 2) Indexar chunks no ChromaDB (Fase 2)

```bash
python retriever.py --chunks-json ./chunks_auditoria.json --limpar
```

> Dica: ajuste o tamanho de lote de indexa√ß√£o (padr√£o 50) com `--lote-indexacao` quando precisar otimizar estabilidade de embeddings.

### 3) (Opcional) Testar resposta via CLI (Fase 3)

```bash
python agent.py --pergunta "Qual √© a vig√™ncia da norma X?"
```

Tamb√©m √© poss√≠vel controlar o lote de indexa√ß√£o no fluxo da Fase 3 com `--lote-indexacao 50`.

### 4) Subir a interface web (Fase 4 + Auditoria visual da Fase 5)

```bash
streamlit run app.py
```

Depois, abra no navegador o endere√ßo mostrado pelo Streamlit (normalmente `http://localhost:8501`).

### 5) Rodar avaliador em lote (Fase 5)

Crie um arquivo `perguntas.txt` com uma pergunta por linha e execute:

```bash
python avaliador_em_lote.py
```

Sa√≠da padr√£o: `relatorio_avaliacao.csv`.


### 6) Auditar manualmente o relat√≥rio no Streamlit

1. Na barra lateral, selecione **Auditoria de Lote** em **Navega√ß√£o**.
2. O app carregar√° `relatorio_avaliacao.csv` automaticamente.
3. Edite apenas a coluna **Avalia√ß√£o Manual** usando as op√ß√µes:
   - (vazio)
   - üëç Correto
   - üëé Incorreto
4. Clique em **Salvar Avalia√ß√µes** para sobrescrever o CSV com suas marca√ß√µes.

---


### Prompts externos especializados

A pasta `prompts/` centraliza os prompts de sistema e elimina textos fixos no c√≥digo Python:

- `especialista_habitacional.txt` (padr√£o da Fase 3)
- `especialista_renda.txt`
- `reescritor_tecnico.txt` (Fase 6)

Para trocar o especialista na gera√ß√£o final (Fase 3), use:

```bash
python agent.py --pergunta "Minha pergunta" --prompt-sistema especialista_renda.txt
```

## Como usar o chat

1. Abra o app com `streamlit run app.py`.
2. Na **sidebar**, ajuste configura√ß√µes como diret√≥rio do Chroma, cole√ß√£o e modelos do Ollama.
3. O campo **Top-K de contexto** inicia em `4` por padr√£o (para reduzir lat√™ncia); diminua para `3` se quiser ainda mais velocidade.
4. Digite sua pergunta no campo de chat. Antes da busca, o sistema aplica automaticamente Query Rewriting para transformar a pergunta em termos t√©cnicos e melhorar a recupera√ß√£o de contexto.
5. Ap√≥s cada resposta, clique em:
   - **üëç Correto** quando a resposta estiver adequada.
   - **üëé Impreciso** quando estiver incorreta ou incompleta.
6. A sidebar atualiza o **Gr√°fico de Aprendizado** com a taxa de acerto (%) por data.

---

## Banco de feedback (`feedback.db`)

A tabela `feedback` armazena:

- `data_hora` (timestamp da avalia√ß√£o)
- `pergunta`
- `resposta`
- `feedback` (`1` para üëç e `0` para üëé)
- `message_id` (identificador √∫nico da resposta para evitar duplicidade)

Esse banco √© criado automaticamente na primeira execu√ß√£o do `app.py`.

---

## Estrutura dos arquivos principais

- `ingest_docx.py` ‚Äî ingest√£o e chunking de `.docx`
- `retriever.py` ‚Äî indexa√ß√£o e busca h√≠brida (vetorial + BM25)
- `agent.py` ‚Äî gera√ß√£o final de resposta com Ollama e carregamento de prompt externo
- `app.py` ‚Äî interface Streamlit com dois modos: **Chatbot** e **Auditoria de Lote**, incluindo edi√ß√£o/salvamento da coluna `Avalia√ß√£o Manual` no CSV
- `avaliador_em_lote.py` ‚Äî execu√ß√£o em lote para valida√ß√£o e auditoria de respostas
- `query_rewriter.py` ‚Äî reescrita t√©cnica de perguntas com prompt externo (Query Rewriting)
- `prompts/` ‚Äî prompts de sistema especializados por dom√≠nio
- `feedback.db` ‚Äî banco SQLite gerado em runtime
- `perguntas.txt` ‚Äî arquivo de entrada (uma pergunta por linha) para a Fase 5
- `relatorio_avaliacao.csv` ‚Äî relat√≥rio gerado pela Fase 5

---

## Solu√ß√£o de problemas

- **Erro ao conectar no Ollama**
  - Verifique se o Ollama est√° ativo e acess√≠vel em `http://localhost:11434`.
- **Sem resultados na busca**
  - Reindexe com `--limpar` para reconstruir a base vetorial e BM25.
- **Gr√°fico de aprendizado vazio**
  - √â esperado at√© existir pelo menos um feedback registrado.

---

## Pr√≥ximas evolu√ß√µes recomendadas

- Filtro por cole√ß√£o/documento no chat.
- Dashboard com distribui√ß√£o de feedback por tema.
- Exporta√ß√£o de feedback para CSV e rotinas de melhoria cont√≠nua do prompt.

---

## Melhorias recentes de intelig√™ncia, performance e robustez

- **Mais r√°pido:** adicionado cache de Query Rewriting para perguntas repetidas, evitando chamadas redundantes ao Ollama.
- **Mais robusto:** valida√ß√µes extras no retriever h√≠brido (`k_rrf > 0` e ao menos um peso de busca maior que zero).
- **Mais resiliente:** alinhamento entre documentos, IDs e metadados no BM25 para evitar inconsist√™ncias quando houver itens inv√°lidos.
- **Mais previs√≠vel para o usu√°rio:** quando nenhum contexto √© recuperado no chat, o sistema retorna imediatamente `[Informa√ß√£o n√£o encontrada no documento]`.
